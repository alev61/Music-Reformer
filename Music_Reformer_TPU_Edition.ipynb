{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Music_Reformer_TPU_Edition.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asigalov61/Music-Reformer/blob/main/Music_Reformer_TPU_Edition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgn4KUAATtXS"
      },
      "source": [
        "# Music Reformer (v.1.5): TPU Edition\n",
        "\n",
        "### This is a work in progress so please check back for updates and improvements.\n",
        "\n",
        "***\n",
        "\n",
        "### Based on the offical Reformer Google Colab and code.\n",
        "\n",
        "https://github.com/google/trax/tree/master/trax/models/reformer\n",
        "\n",
        "***\n",
        "\n",
        "Project Los Angeles\n",
        "\n",
        "Tegridy Code 2021\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxJaH9ga8bQr"
      },
      "source": [
        "# Setup the environment\n",
        "\n",
        "### Please note that you may need to run the cells below several times, as well as you may need to restart Colab run-time also several times to resolve all dependencies conflicts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QINqPZH6FzB3",
        "cellView": "form"
      },
      "source": [
        "#@title Install the dependencies\n",
        "# Install dependencies\n",
        "\n",
        "!git clone https://github.com/asigalov61/tegridy-tools\n",
        "%cd /content/tegridy-tools/tegridy-tools/\n",
        "%cd /content/\n",
        "\n",
        "#!wget https://github.com/asigalov61/Music-Reformer/raw/main/Dataset/Music-Reformer_TXT_Dataset.zip\n",
        "#!unzip Music-Reformer_TXT_Dataset.zip\n",
        "\n",
        "!pip install --upgrade -q jax\n",
        "!pip install --upgrade -q jaxlib\n",
        "!pip install --upgrade -q trax==1.3.6\n",
        "!pip install --upgrade -q sentencepiece\n",
        "!pip install --upgrade -q gin "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pv0PuxC3F-jn",
        "cellView": "form"
      },
      "source": [
        "#@title Import modules\n",
        "\n",
        "print('Loading needed modules. Please wait...')\n",
        "\n",
        "# Make sure the Colab Runtime is set to Accelerator: TPU.\n",
        "import requests\n",
        "import os\n",
        "if 'TPU_DRIVER_MODE' not in globals():\n",
        "  url = 'http://' + os.environ['COLAB_TPU_ADDR'].split(':')[0] + ':8475/requestversion/tpu_driver0.1-dev20191206'\n",
        "  resp = requests.post(url)\n",
        "  TPU_DRIVER_MODE = 1\n",
        "\n",
        "# The following is required to use TPU Driver as JAX's backend.\n",
        "from jax.config import config\n",
        "config.FLAGS.jax_xla_backend = \"tpu_driver\"\n",
        "config.FLAGS.jax_backend_target = \"grpc://\" + os.environ['COLAB_TPU_ADDR']\n",
        "print(config.FLAGS.jax_backend_target)\n",
        "\n",
        "import gin\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from scipy.special import softmax\n",
        "\n",
        "import tqdm\n",
        "from tqdm import auto\n",
        "\n",
        "# NLP Vocab Generation\n",
        "import sentencepiece as spm\n",
        "\n",
        "%cd /content/tegridy-tools/tegridy-tools\n",
        "import TMIDI\n",
        "%cd /content/\n",
        "\n",
        "import os\n",
        "\n",
        "if not os.path.exists('/content/Dataset'):\n",
        "    os.makedirs('/content/Dataset')\n",
        "\n",
        "# Zipping and downloading files\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "# Trax\n",
        "import jax\n",
        "import trax\n",
        "from trax.data import inputs\n",
        "import jax.numpy as jnp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7OJzWNu8YG9"
      },
      "source": [
        "# Prep the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "VXWcNRZQqrVn"
      },
      "source": [
        "#@title Download special Tegridy Piano MIDI dataset\n",
        "\n",
        "#@markdown Works best stand-alone/as-is for the optimal results\n",
        "%cd /content/Dataset/\n",
        "\n",
        "!wget 'https://github.com/asigalov61/Tegridy-MIDI-Dataset/raw/master/Tegridy-Piano-CC-BY-NC-SA.zip'\n",
        "!unzip -j '/content/Dataset/Tegridy-Piano-CC-BY-NC-SA.zip'\n",
        "!rm '/content/Dataset/Tegridy-Piano-CC-BY-NC-SA.zip'\n",
        "\n",
        "%cd /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWYnKxz2ZeVK",
        "cellView": "form"
      },
      "source": [
        "#@title Process MIDIs to special MIDI dataset with Tegridy MIDI Processor\n",
        "#@markdown NOTES:\n",
        "\n",
        "#@markdown 1) Dataset MIDI file names are used as song names. Feel free to change it to anything you like.\n",
        "\n",
        "#@markdown 2) Best results are achieved with the single-track, single-channel, single-instrument MIDI 0 files with plain English names (avoid special or sys/foreign chars)\n",
        "\n",
        "#@markdown 3) MIDI Channel = -1 means all MIDI channels. MIDI Channel = 16 means all channels will be processed. Otherwise, only single indicated MIDI channel will be processed.\n",
        "\n",
        "file_name_to_output_dataset_to = \"/content/Music-Reformer_TXT_Dataset\" #@param {type:\"string\"}\n",
        "desired_MIDI_channel_to_process = 0 #@param {type:\"slider\", min:-1, max:15, step:1}\n",
        "encode_velocities = True #@param {type:\"boolean\"}\n",
        "chordify_input_MIDIs = False #@param {type:\"boolean\"}\n",
        "time_denominator = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "chars_encoding_offset = 33 #@param {type:\"number\"}\n",
        "\n",
        "print('TMIDI Processor')\n",
        "print('Starting up...')\n",
        "\n",
        "###########\n",
        "\n",
        "average_note_pitch = 0\n",
        "min_note = 127\n",
        "max_note = 0\n",
        "\n",
        "files_count = 0\n",
        "\n",
        "ev = 0\n",
        "\n",
        "chords_list_f = []\n",
        "melody_list_f = []\n",
        "\n",
        "chords_list = []\n",
        "chords_count = 0\n",
        "\n",
        "melody_chords = []\n",
        "melody_count = 0\n",
        "\n",
        "TXT_String = 'DATASET=Optimus-Virtuoso-Music-Dataset' + chr(10)\n",
        "\n",
        "TXT = ''\n",
        "melody = []\n",
        "chords = []\n",
        "\n",
        "###########\n",
        "\n",
        "print('Loading MIDI files...')\n",
        "print('This may take a while on a large dataset in particular.')\n",
        "\n",
        "dataset_addr = \"/content/Dataset/\"\n",
        "os.chdir(dataset_addr)\n",
        "filez = os.listdir(dataset_addr)\n",
        "\n",
        "print('Processing MIDI files. Please wait...')\n",
        "for f in tqdm.auto.tqdm(filez):\n",
        "  try:\n",
        "\n",
        "    files_count += 1\n",
        "    TXT, melody, chords = TMIDI.Optimus_MIDI_TXT_Processor(f, chordify_TXT=chordify_input_MIDIs, output_MIDI_channels=False, char_offset=chars_encoding_offset, dataset_MIDI_events_time_denominator=time_denominator, output_velocity=encode_velocities, MIDI_patch=range(0,127))\n",
        "    melody_list_f += melody\n",
        "    chords_list_f += chords\n",
        "    TXT_String += TXT\n",
        "    \n",
        "  \n",
        "  except:\n",
        "    print('Bad MIDI:', f)\n",
        "    continue\n",
        "\n",
        "print('Task complete :)')\n",
        "print('==================================================')\n",
        "print('Number of processed dataset MIDI files:', files_count)\n",
        "print('Number of MIDI chords recorded:', len(chords_list_f))\n",
        "print('First chord event:', chords_list_f[0], 'Last chord event:', chords_list_f[-1]) \n",
        "print('Number of recorded melody events:', len(melody_list_f))\n",
        "print('First melody event:', melody_list_f[0], 'Last Melody event:', melody_list_f[-1])\n",
        "print('Total number of MIDI events recorded:', len(chords_list_f) + len(melody_list_f))\n",
        "\n",
        "# Writing dataset to TXT file\n",
        "with open(file_name_to_output_dataset_to + '.txt', 'wb') as f:\n",
        "  f.write(TXT_String.encode('utf-8', 'replace'))\n",
        "  f.close\n",
        "\n",
        "# Dataset\n",
        "MusicDataset = [chords_list_f, melody_list_f]\n",
        "\n",
        "# Writing dataset to pickle file\n",
        "TMIDI.Tegridy_Pickle_File_Writer(MusicDataset, file_name_to_output_dataset_to)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7shBqm0T6rz",
        "cellView": "form"
      },
      "source": [
        "#@title Process the TXT MIDI dataset to TXT INT dataset\n",
        "full_path_to_TXT_dataset = \"/content/Music-Reformer_TXT_Dataset.txt\" #@param {type:\"string\"}\n",
        "with open(full_path_to_TXT_dataset, 'r') as file:\n",
        "  z = file.read()\n",
        "  file.close()\n",
        "  Z = z.encode('utf8')\n",
        "  Y = list(Z)\n",
        "\n",
        "string = '\\n'.join([str(item) for item in Y])\n",
        "\n",
        "with open('/content/Music-Reformer_INT_Dataset.txt', 'w') as file:\n",
        "  file.write(string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_l9Znw2bS-Pg",
        "cellView": "form"
      },
      "source": [
        "#@title Create a tokenizer and its model\n",
        "\n",
        "#@markdown NOTE: Less tokenizer words seem to work better\n",
        "\n",
        "%cd /content/\n",
        "\n",
        "full_path_to_INT_dataset = \"/content/Music-Reformer_INT_Dataset.txt\" #@param {type:\"string\"}\n",
        "tokenizer_vocabulary_size_in_words =  321#@param {type:\"integer\"}\n",
        "\n",
        "# Train a BPE model on the dataset\n",
        "spm.SentencePieceTrainer.train(input=full_path_to_INT_dataset,\n",
        "                              model_prefix='Music-Reformer-Tokenizer',\n",
        "                              vocab_size=tokenizer_vocabulary_size_in_words,\n",
        "                              model_type='bpe')\n",
        "# Load BPE vocabulary\n",
        "TOKENIZER = spm.SentencePieceProcessor() \n",
        "TOKENIZER.load('Music-Reformer-Tokenizer.model')\n",
        "\n",
        "# Load the dataset\n",
        "with open(full_path_to_INT_dataset, 'r') as f:\n",
        "    text = f.read(512 * 3072)\n",
        "\n",
        "\n",
        "IDS = TOKENIZER.EncodeAsIds(text)\n",
        "IDS = np.asarray(IDS, dtype=np.int32)\n",
        "PAD_AMOUNT = 512 * 1024 - len(IDS)\n",
        "print(\"Number of tokens:\", IDS.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQwhEqJzS-I2",
        "cellView": "form"
      },
      "source": [
        "#@title Split the dataset\n",
        "train_validation_split_ratio = 0.9 #@param {type:\"slider\", min:0.05, max:0.95, step:0.05}\n",
        "\n",
        "# Tokenize (set to max for the provided dataset)\n",
        "trX, vaX = np.split(Y[:512 * 1024], [int((len(Y[:512 * 1024]) * train_validation_split_ratio))])\n",
        "data_train, data_val = torch.from_numpy(trX), torch.from_numpy(vaX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glkNjfVX8TKL"
      },
      "source": [
        "# Setup the Reformer model and functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdKnx2FyS-AO",
        "cellView": "form"
      },
      "source": [
        "#@title Initialize the functions and procedures for training\n",
        "# Set up the data pipeline.\n",
        "def my_inputs(n_devices):\n",
        "  while True:\n",
        "    inputs = []\n",
        "    mask = []\n",
        "    pad_amounts = np.random.choice(PAD_AMOUNT, n_devices)\n",
        "    for i in range(n_devices):\n",
        "      inputs.append(np.pad(IDS, (pad_amounts[i], PAD_AMOUNT - pad_amounts[i]), # Pad IDS by different amount for each device\n",
        "                            mode='constant'))\n",
        "      mask.append(np.pad(np.ones_like(IDS, dtype=np.float32),\n",
        "                          (pad_amounts[i], PAD_AMOUNT - pad_amounts[i]),\n",
        "                          mode='constant'))\n",
        "    inputs = np.stack(inputs)\n",
        "    mask = np.stack(mask)\n",
        "    yield (inputs, inputs, mask)\n",
        "\n",
        "print(\"(device count, tokens per device) = \",\n",
        "      next(my_inputs(trax.fastmath.device_count()))[0].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NeFGlGGS959",
        "cellView": "form"
      },
      "source": [
        "#@title Configure hyperparamenters\n",
        "# Configure hyperparameters.\n",
        "gin.parse_config(\"\"\"\n",
        "import trax.layers\n",
        "import trax.models\n",
        "import trax.optimizers\n",
        "import trax.data.inputs\n",
        "import trax.supervised.trainer_lib\n",
        "\n",
        "# Parameters that will vary between experiments:\n",
        "# ==============================================================================\n",
        "train.model = @trax.models.ReformerLM\n",
        "# Model will have 6 layers, alternating between the LSH attention\n",
        "# and local attention within a certain context window.\n",
        "n_layers = 6\n",
        "attn_type = [\n",
        "  @trax.layers.SelfAttention,\n",
        "  @LSHSelfAttention,  \n",
        "  @trax.layers.SelfAttention,\n",
        "  @LSHSelfAttention,\n",
        "  @trax.layers.SelfAttention,\n",
        "  @LSHSelfAttention,\n",
        "  ]\n",
        "share_qk = False  # LSH attention ignores this flag and always shares q & k\n",
        "n_heads = 2\n",
        "attn_kv = 64\n",
        "dropout = 0.05\n",
        "n_tokens = 524288\n",
        "\n",
        "# Parameters for multifactor:\n",
        "# ==============================================================================\n",
        "multifactor.constant = 0.01\n",
        "multifactor.factors = 'constant * linear_warmup * cosine_decay'\n",
        "multifactor.warmup_steps = 100\n",
        "multifactor.steps_per_cycle = 900\n",
        "\n",
        "# Parameters for Adam:\n",
        "# ==============================================================================\n",
        "Adam.weight_decay_rate=0.0\n",
        "Adam.b1 = 0.86\n",
        "Adam.b2 = 0.92\n",
        "Adam.eps = 1e-9\n",
        "\n",
        "# Parameters for SelfAttention:\n",
        "# ==============================================================================\n",
        "trax.layers.SelfAttention.attention_dropout = 0.05\n",
        "trax.layers.SelfAttention.chunk_len = 64\n",
        "trax.layers.SelfAttention.n_chunks_before = 1\n",
        "trax.layers.SelfAttention.n_parallel_heads = 1\n",
        "\n",
        "# Parameters for LSHSelfAttention:\n",
        "# ==============================================================================\n",
        "LSHSelfAttention.attention_dropout = 0.0\n",
        "LSHSelfAttention.chunk_len = 64\n",
        "LSHSelfAttention.n_buckets = [64, 128]\n",
        "LSHSelfAttention.n_chunks_after = 0\n",
        "LSHSelfAttention.n_chunks_before = 1\n",
        "LSHSelfAttention.n_hashes = 1\n",
        "LSHSelfAttention.n_parallel_heads = 1\n",
        "LSHSelfAttention.predict_drop_len = 128\n",
        "LSHSelfAttention.predict_mem_len = 1024\n",
        "\n",
        "# Parameters for ReformerLM:\n",
        "# ==============================================================================\n",
        "ReformerLM.attention_type = %attn_type\n",
        "ReformerLM.d_attention_key = %attn_kv\n",
        "ReformerLM.d_attention_value = %attn_kv\n",
        "ReformerLM.d_model = 256\n",
        "ReformerLM.d_ff = 512\n",
        "ReformerLM.dropout = %dropout\n",
        "ReformerLM.ff_activation = @trax.layers.Relu\n",
        "ReformerLM.max_len = %n_tokens\n",
        "ReformerLM.mode = 'train'\n",
        "ReformerLM.n_heads = %n_heads\n",
        "ReformerLM.n_layers = %n_layers\n",
        "ReformerLM.vocab_size = 320\n",
        "ReformerLM.axial_pos_shape = (512, 1024)\n",
        "ReformerLM.d_axial_pos_embs= (64, 192)\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8ay-6VxS9xw",
        "cellView": "form"
      },
      "source": [
        "#@title Setup the model and the trainer routines\n",
        "# Trainer.\n",
        "output_dir = os.path.expanduser('model')\n",
        "#!rm -f ~/model/model.pkl.gz  # Remove old model\n",
        "\n",
        "trainer = trax.supervised.Trainer(\n",
        "    model=trax.models.ReformerLM,\n",
        "    loss_fn=trax.layers.CrossEntropyLoss(),\n",
        "    optimizer=trax.optimizers.Adam,\n",
        "    lr_schedule=trax.lr.multifactor(),\n",
        "    inputs=trax.data.inputs.Inputs(my_inputs),\n",
        "    output_dir=output_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeMJKx6P8Puc"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qf0HITY7Wz8C",
        "cellView": "form"
      },
      "source": [
        "#@title Train the model\n",
        "# Train Model\n",
        "\n",
        "#@markdown This cell takes about 10 minutes to produce first output. Please wait...\n",
        "import tqdm\n",
        "print('=' * 50)\n",
        "print('JITing NN...')\n",
        "trainer.train_epoch(n_steps=1, n_eval_steps=1)\n",
        "print('=' * 50)\n",
        "print('Continuing last run to the max...')\n",
        "trainer.train_epoch(n_steps=9, n_eval_steps=1)\n",
        "print('=' * 50)\n",
        "\n",
        "print('Running main training loop')\n",
        "for _ in tqdm.auto.tqdm(range(59)):\n",
        "  trainer.train_epoch(n_steps=10, n_eval_steps=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEikLctyte-m",
        "cellView": "form"
      },
      "source": [
        "#@title Zip and download your trained model checkpoint here\n",
        "# Zip directory contents\n",
        "shutil.make_archive(\"project\", \"zip\", \".\")\n",
        "\n",
        "# Download zipped directory\n",
        "files.download('project.zip')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1vc_Pa_8JGj"
      },
      "source": [
        "# Generate Music"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLebeiAFrItK",
        "cellView": "form"
      },
      "source": [
        "#@title Increase hashing rounds number for better quality here\n",
        "# In the Reformer paper, increasing the number of hashing rounds helps with quality. \n",
        "# The number of hashing rounds at can be increased at evaluation time only.\n",
        "gin.parse_config(\"\"\"LSHSelfAttention.n_hashes = 4\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "cquc4SJD0Y_J"
      },
      "source": [
        "#@title Load the trained Reformer in 'predict' mode\n",
        "# Load the trained Reformer in 'predict' mode\n",
        "model = trax.models.ReformerLM(mode='predict')\n",
        "output_dir = os.path.expanduser('model')\n",
        "model.init_from_file(os.path.join(output_dir,'model.pkl.gz'),\n",
        "                     weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thdiRvGDrJhy",
        "cellView": "form"
      },
      "source": [
        "#@title Generate and decode music from the model\n",
        "# Sample from ReformerLM\n",
        "output_token_ids = trax.supervised.decoding.autoregressive_sample(\n",
        "    model, temperature=0.8, max_length=2048, batch_size = 1)\n",
        "\n",
        "# Decode token IDs\n",
        "# Reformer outputed a batch with one item so access it using [0]\n",
        "# tolist() converts from int64 to int, the type SentencePiece expects\n",
        "input = TOKENIZER.DecodeIds(output_token_ids[0].tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ox03iX3KHCKa",
        "cellView": "form"
      },
      "source": [
        "#@title Convert generated output to MIDI.\n",
        "# Run the cells below to convert generated output to MIDI.\n",
        "# If you getting errors/halts, regenerate the output again.\n",
        "# Model must be sufficiently trained. Rec. 0.90+ accuracy for the output to make sense and pass error control.\n",
        "\n",
        "#TXT = TMIDI.Tegridy_INT_String_to_TXT_Converter(input, line_by_line_input=False)\n",
        "SONG = TMIDI.Tegridy_Optimus_TXT_to_Notes_Converter(input, has_MIDI_channels=False, char_encoding_offset=30000, simulate_velocity=True, dataset_MIDI_events_time_denominator=1, line_by_line_dataset=False)\n",
        "stats = TMIDI.Tegridy_SONG_to_MIDI_Converter(SONG=SONG[0], output_file_name='/content/Music-Reformer_MIDI')\n",
        "print(stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7RAK2XE8jdE"
      },
      "source": [
        "# Congrats!!! You did it!!! :)"
      ]
    }
  ]
}